{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAui1F5Youxz"
      },
      "source": [
        "<center><img src=\"data:image/png;base64,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\" title=\"Title text\" /></center>\n",
        "\n",
        "\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "<h1 align='center'> INF-398 INTRODUCCIÓN AL APRENDIZAJE AUTOMÁTICO - 2023-1 </h1>\n",
        "\n",
        "<H3 align='center'> Tarea 1  </H3>\n",
        "<hr style=\"height:2px;border:none\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afvVkQDipqL3"
      },
      "source": [
        "**Temas**  \n",
        "* Manipulaciones en pandas y numpy, imputación de datos y preprocesamientos\n",
        "* Feature Engineering\n",
        "* Regresión\n",
        "* Clasificación\n",
        "* Regularización\n",
        "* SVM\n",
        "\n",
        "\n",
        "**Formalidades**  \n",
        "* Equipos de trabajo de 3 personas (*Los estudiantes deben estar preparados para presentar la tarea el día de la entrega*)\n",
        "* El entregable debe ser un _Jupyter Notebook_ incluyendo los códigos utilizados, los resultados, los gráficos realizados y comentarios. Debe seguir una estructura similar a un informe (se debe introducir los problemas a trabajar, presentar los resultados y discutirlos), se penalizará fuertemente ausencia de comentarios, explicaciones de gráficos, _etc_. Las preguntas que deben responder se encuentran especificadas explícitamente con letras, ejemplo a) b) c), por lo tanto cualquier pregunta que se encuentre incompleta será penalizada con descuento del puntaje. Recuerde que los códigos presentes en este documento son solo una guía de referencia, por lo tanto no necesariamente funcionan, usted debe mostrar todos sus conocimientos aprendidos a lo largo de la carrera para que su código funcione como se espera. Si lo prefiere puede entregar un _Jupyter Notebook_ por pregunta o uno por toda la tarea, con tal de que todos los entregables esten bien identificados y se encuentren en el mismo repositorio de _Github_.\n",
        "* Se debe preparar una presentación del trabajo realizado y sus hallazgos. El presentador será elegido aleatoriamente y deberá apoyarse en el _Jupyter Notebook_ que entregarán. \n",
        "* Formato de entrega: envı́o de link del repositorio en _Github_, al correo electrónico de los ayudantes (<sebastian.sanchezl@sansano.usm.cl>), en copia al profesor (<cvalle@inf.utfsm.cl>). Especificar el siguiente asunto: [INF-398-2023-1 Tarea 1]. Invitar como colaborador a los usuarios de github \"ssanchezl\" para poder acceder al repositorio en caso de ser privado.\n",
        "\n",
        "* Fecha de presentaciones 12 de Mayo, en horario de clases.\n",
        "* Fecha de entrega: 13 de Mayo. Hora límite de entrega: 12:00 p.m. Cualquier _commit_ luego de la hora límite no será evaluado. Se realizará descuento por atrasos en envío del mail igualmente.  \n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma71e6Q9WQ3v"
      },
      "source": [
        "Importe las librerías necesarias para cargar explorar el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PYCL-EyLDn0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgOo-Ccfr4kc"
      },
      "source": [
        "# 1. Regresión lineal para la predicción de la demanda de bicicletas compartidas en Seúl\n",
        "\n",
        "La regresión lineal es un modelo matemático que pertenece al aprendizaje supervisado, el cual intenta aplicar relaciones que predigan el resultado de un evento basándose en los datos de variables independientes. Llamaremos a este resultado: variable objetivo o _target_, y como es un modelo de regresión lineal la variable objetivo es una variable continua.\n",
        "\n",
        "<center><img src=https://t4.ftcdn.net/jpg/02/32/47/13/360_F_232471334_vfDHKHeRHtwkn0zvd8SM98THwQeYDn8y.jpg></center>\n",
        "\n",
        "Los sistemas de bicicletas compartidas son un medio para alquilar bicicletas en el que el proceso de afiliación, alquiler y devolución se automatiza a través de una red de _quioscos o puntos de estacionamiento_ repartidos por toda la ciudad. Gracias a estos sistemas, los usuarios pueden alquilar una bicicleta en un lugar y devolverla en otro cuando lo necesiten.\n",
        "\n",
        "En esta pregunta se le proporcionan datos de alquiler de la cantidad de bicicletas arrendadas por hora durante dos años según variables climáticas y laborales. \n",
        "\n",
        "Usted tiene separar el dataset en un conjunto de entrenamiento y en un conjunto de prueba, y el objetivo de esta pregunta será: **predecir el número total de bicicletas alquiladas en el conjunto de pruebas**.\n",
        "\n",
        "La base de datos y toda la información asociada a esta se puede encontrar en el siguiente link: https://www.kaggle.com/datasets/saurabhshahane/seoul-bike-sharing-demand-prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwXkfql_NnIh"
      },
      "source": [
        "## 1.1 Manipulación y exploración del conjunto de datos\n",
        "\n",
        "Lo primero es obtener los datos, para esto se pueden descargar los archivos directamente de la página de Kaggle a través del link de la descripción de arriba.\n",
        "\n",
        "Existen distintas formas para obtener los datos, de las cuales usted debe elegir solo uno:\n",
        "\n",
        "1. Puede descargarlos y guardarlos en su directorio local y luego cargar los datos usando este notebook con Jupyter.\n",
        "\n",
        "2. Puede utilizar google colab. En este caso existen 2 formas de cargar los datos, montando su cuenta de google drive en el colab y guardar los datos en una carpeta de drive. La otra forma es cargar los datos de forma en la máquiva virtual que le asigna colab de manera **temporal**. Tenga cuidado si elige la forma temporal ya que si por alguna razón se desconecta del ambiente, deberá volver a descargar los datos.\n",
        "\n",
        "3. Existen muchas otras formas más pero usted puede obtener los datos con cualquier método que estime conveniente.\n",
        "\n",
        "\n",
        "También es posible descargarlos mediante la [API](https://github.com/Kaggle/kaggle-api#kaggle-api) de Kaggle siguiendo las instrucciones en la sección API credentials. Si desea utilizar google colab puede seguir este tutorial [Kaggle + Colab](https://galhever.medium.com/how-to-import-data-from-kaggle-to-google-colab-8160caa11e2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwrAvQkg61WQ"
      },
      "source": [
        "Localizar y subir la api-key de kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "TWqskVWeHyyO",
        "outputId": "ae878db6-aae8-4e37-e2f4-dfd4edda7a1e"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2sQwOqZ67zx"
      },
      "source": [
        "Otorgar permisos de lectura y escritura al archivo kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5x4CoYdKWTL"
      },
      "outputs": [],
      "source": [
        "#!mkdir -p ~/.kaggle\n",
        "#!cp kaggle.json ~/.kaggle/\n",
        "#!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkCJq4h67QBE"
      },
      "source": [
        "Descargar el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyWjZD3L_PCv",
        "outputId": "2f433769-b0a7-4367-ec5e-c21d3271ce36"
      },
      "outputs": [],
      "source": [
        "#!kaggle datasets download -d saurabhshahane/seoul-bike-sharing-demand-prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7DIRNYy7UrH"
      },
      "source": [
        "Descomprimir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvRKiEnzU5in",
        "outputId": "a8a68a8c-bdee-45f0-bbfd-4affb8d3b0ad"
      },
      "outputs": [],
      "source": [
        "#!unzip seoul-bike-sharing-demand-prediction.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz9zyBxkXgdB"
      },
      "source": [
        "### 1.1.1 Exploración del conjunto de datos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ueS8iujKrSc"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('SeoulBikeData.csv',encoding= 'unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLUKTAS7KP_Y"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf6OuaXU6lAv"
      },
      "source": [
        "Explore el conjunto de datos y responda  brevemente las siguiente preguntas:\n",
        "\n",
        "> a) ¿Cuáles variables poseen valores numéricos y cuáles poseen valores categóricos?\n",
        "\n",
        "> b) ¿Qué aspecto tienen en común las variables _Date_ y _Season_?\n",
        "\n",
        "> c) Revise si faltan valores en los conjuntos de datos de prueba y de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a)\n",
        "-----\n",
        "#### Las variables que poseen valores numericos son:\n",
        "- Rented Bike Count\n",
        "- Hour\n",
        "- Temperature\n",
        "- Humidity\n",
        "- Wind speed\n",
        "- Visibility\n",
        "- Drew point temperature\n",
        "- Solar Radiation (MJ/m2)\n",
        "- Rainfall(mm)\n",
        "- Snowfall (cm)\n",
        "- Date\n",
        "#### Las variables que poseen valores categoricos:\n",
        "- Seasons\n",
        "- Holiday\n",
        "- Functioning Day\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b)\n",
        "----\n",
        "\n",
        " El aspecto en comun de estas variables es el tiempo. La variable Season nos entrega informacion de la estacion del año, que puede ser util para analizar la influencia de las condiciones climaticas en diferentes estaciones del año. Por otro lado, tenemos la variable date que puede ser util para tendencias a lo largo del tiempo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c)\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItJ9oQzALZHU"
      },
      "outputs": [],
      "source": [
        "def datainfo():\n",
        "    temp_ps = pd.DataFrame(index=data.columns)\n",
        "    temp_ps['DataType'] = data.dtypes\n",
        "    temp_ps[\"Non-null_Values\"] = data.count()\n",
        "    temp_ps['Unique_Values'] = data.nunique()\n",
        "    temp_ps['NaN_Values'] = data.isnull().sum()\n",
        "    temp_ps['NaN_Values_Percentage'] = (temp_ps['NaN_Values']/len(data))*100 \n",
        "    return temp_ps\n",
        "\n",
        "datainfo()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1THsYsFDQBQr"
      },
      "source": [
        "> d) ¿Por qué la variable _Functioning Day_ no aporta ningún valor al entrenamiento? Grafique, justifique y elimine la columna.\n",
        "\n",
        "ESTA PREGUNTA FUE ELIMINADA SEGUN EL AYUDANTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHMABW7OBjdf"
      },
      "outputs": [],
      "source": [
        "def barplots(x,y,hue):\n",
        "    plt.figure(figsize=(15,7))\n",
        "    sns.set_palette(\"husl\")\n",
        "    sns.barplot(x=x,y=y,hue=hue,data=data);\n",
        "barplots('Hour','Rented Bike Count','Functioning Day')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD0t2NqvQoio"
      },
      "outputs": [],
      "source": [
        "#data.drop('Functioning Day', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1DmRepKxGmC"
      },
      "source": [
        "### 1.1.2 Exploración de Features\n",
        "\n",
        "> a) Revise detalladamente las variables: 'Date', 'Holiday', 'Seasons' y 'Rented Bike Count' del conjunto de datos cuente y grafique su histograma.\n",
        "\n",
        "Una vez analizado lo anterior responda las siguientes preguntas:\n",
        "\n",
        "> b) ¿La gente arrienda más bicicletas en vacaciones o en días de trabajo?\n",
        "\n",
        "> c) ¿Se arriendan más bicicletas los fines de semana o durante la semana?\n",
        "\n",
        "> d) ¿De qué forma afecta el clima al arriendo de bicicletas?\n",
        "\n",
        "Para las preguntas b), c) y d) apóyese de los gráficos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN-iZBaFyIxH"
      },
      "outputs": [],
      "source": [
        "#cat_features = data[['Holiday', 'Seasons', 'Rented Bike Count']]\n",
        "#for i in cat_features:\n",
        "#    ax = sns.countplot(x = i, data = data)\n",
        "#    plt.title(i)\n",
        "#    \"\"\"\n",
        "#    Su código aquí\n",
        "#    \"\"\"\n",
        "#    plt.show()\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a) \n",
        "---\n",
        "\n",
        "Para la variable Date, que es continua, sería más apropiado graficar un histograma de frecuencias. Sin embargo, como la variable Date no es categórica ni numérica discreta, sino continua, puede ser más adecuado representarla mediante un gráfico de línea o de puntos que muestre cómo varía el número de bicicletas arrendadas a lo largo del tiempo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b)\n",
        "---\n",
        "Si suponemos que los dias de trabajo son los dias 'No Holiday' y los dias de no trabajo son los dias 'Holiday', entonces tenemos que segun el siguiente grafico, la gente arrienda mas bicicletas en los dias de trabajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.barplot(x='Holiday',y='Rented Bike Count' , data=data , estimator=sum);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.groupby(by=['Holiday']).sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c)\n",
        "---\n",
        "Los dias de semana son de Lunes a Viernes y los Fin de semana son Sabado y Domingo. Entonces hay que graficar un histograma que compare el arriendo de bicicletas en esos dos rangos de dias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKN9v9mG0qZ5"
      },
      "outputs": [],
      "source": [
        "#ax = sns.barplot(data=data, x='Date', y='Rented Bike Count')\n",
        "\n",
        "def isWeekend(day):\n",
        "    if(day > 4):\n",
        "        return \"Fin de semana\"\n",
        "    return \"Dia semana\"\n",
        "\n",
        "is_wekeend = data[ ['Date', 'Rented Bike Count'] ].copy()\n",
        "is_wekeend['day_of_week'] =  (pd.to_datetime( is_wekeend['Date'] ).dt.dayofweek).apply( lambda x : isWeekend(x) )\n",
        "sns.barplot(data=is_wekeend, x='day_of_week', y='Rented Bike Count', estimator=sum)\n",
        "\n",
        "print(\"Dia semana: \" , is_wekeend[ is_wekeend['day_of_week'] == \"Dia semana\" ]['Rented Bike Count'].sum()    )\n",
        "print(\"Fin de semana: \" , is_wekeend[ is_wekeend['day_of_week'] == \"Fin de semana\" ]['Rented Bike Count'].sum() )\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Segun el grafico anterior la gente arrienda mas bicicletas los dias de semana"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d)\n",
        "---\n",
        "Segun el siguiente grafico, la gente tiende a arrendar mas bicicletas en las estaciones mas calurosas y a disminuir su arriendo en las estaciones mas frias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax = sns.barplot(data=data, x='Seasons', y='Rented Bike Count', estimator=sum)\n",
        "ax.bar_label(ax.containers[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHhuVI7N0j9t"
      },
      "source": [
        "> e) Grafique el promedio de bicicletas arrendadas por estación ('Seasons').\n",
        "\n",
        "Responda las siguientes preguntas:\n",
        "\n",
        "> f) ¿Cuáles son las estaciones en las que más se arriendan bicicletas?\n",
        "\n",
        "> g) Transforme la fecha en variables numéricas, es decir, si _Date_ es 01/12/2017, entonces obtenga _Año_=2017, _mes_=12, _día_=1. Agreguelas al dataframe."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e)\n",
        "----\n",
        "El promedio de bicicletas arrendadas corresponderia a la siguiente visualizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax = sns.barplot(data=data, x='Seasons', y='Rented Bike Count', estimator='mean')\n",
        "ax.bar_label(ax.containers[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "f)\n",
        "---\n",
        "\n",
        "Las estaciones que mas arriendan bicicletas estan representadas en orden de mayor a menor en el siguiente listado:\n",
        "- [Summer, Autumn, Spring, Winter]\n",
        "\n",
        "Es decir, summer es la estacion con mas arriendos con un total de 2.28323e+06 arriendos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGaZCXER93ob"
      },
      "outputs": [],
      "source": [
        "data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')\n",
        "\n",
        "data['year'] = data['Date'].dt.year\n",
        "data['month'] = data['Date'].dt.month\n",
        "data['dayofweek'] = data['Date'].dt.dayofweek\n",
        "\n",
        "#data.drop('Date', axis=1, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "g)\n",
        "---\n",
        "La variable date queda descritas de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['Dia'] = pd.to_datetime(data['Date'], format='%d/%m/%Y').dt.day\n",
        "data['Mes'] = pd.to_datetime(data['Date'], format='%d/%m/%Y').dt.month\n",
        "data['Año'] = pd.to_datetime(data['Date'], format='%d/%m/%Y').dt.year\n",
        "data[['Dia','Mes','Año','Date']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rootH3kyYMnl"
      },
      "source": [
        "> h) Grafique y analice de forma general la correlación entre las variables **numéricas**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrxKb6LtYU7H"
      },
      "outputs": [],
      "source": [
        "data_corr= data.corr()       \n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "sns.heatmap(data_corr, cmap='coolwarm', linewidths=0.1, annot=True, linecolor='white')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2wqTI44jeeuY"
      },
      "source": [
        "> i) Escriba sus conclusiones sobre los resultados encontrados en la correlación en la pregunta h)\n",
        ">\n",
        "> Una de las correlaciones importantes que se pueden notar es la del numero de bicicletas rentadas y la temperatura, hay una cierta correlacion positiva entre esas dos variables, lo cual puede ser util para implementar estrategias en torno a la venta de bicicletas para cuando se pronostican altas temperaturas. \n",
        "> \n",
        "> Otra correlacion es la de Visibility y Humidity, con una correlacion negativa. Esta correlacion se puede explicar debido a fenomenos naturales, como la neblina que al fin y al cabo son particulas de agua que estan en el aire (Alta humedad), lo que obstaculizan la vista (Poca visibilidad). Sin embargo, la visibilidad al parecer tiene poco impacto en lo que es el arriendo de bicicletas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meYTfx4TZkXb"
      },
      "source": [
        "### 1.1.3 Separación de conjuntos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g06GKfmsZwAq"
      },
      "source": [
        "> a) Divida el dataset en features _X_ y target _y_, además divida el dataset en un subconjunto de entrenamiento y otro de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQOD1Q7paNHL"
      },
      "outputs": [],
      "source": [
        "X=data.drop('Rented Bike Count',axis=1)\n",
        "y=data['Rented Bike Count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIlvlHlA1dRH"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gjw_L6wh9NBw"
      },
      "source": [
        "> a) ¿Para qué es el parámetro stratify?\n",
        ">\n",
        ">  es para que el conjunto de entrenamiento como el conjunto de prueba tengan una proporción similar de observaciones en cada clase.\n",
        ">  por ejemplo, si la variable y es una variable categórica binaria con valores 0 y 1 y hay un 25% de ceros y un 75% de unos, stratify=y se asegurará de que su división aleatoria tenga un 25% de ceros y un 75% de unos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSOWDLcXWLH4"
      },
      "source": [
        "> b) Aplique Label Encoding a las variables _Seasons_ y _Holiday_ en el conjunto de entrenamiento y responda para qué sirve y como se usa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir variables categóricas\n",
        "#cat_vars = ['Seasons', 'Holiday']\n",
        "\n",
        "# Crear objetos LabelEncoder\n",
        "#le = preprocessing.LabelEncoder()\n",
        "\n",
        "# Aplicar fit_transform en el conjunto de entrenamiento\n",
        "#X_train[cat_vars] = X_train[cat_vars].apply(le.fit_transform)\n",
        "\n",
        "# Aplicar transform en el conjunto de prueba\n",
        "#X_test[cat_vars] = X_test[cat_vars].apply(le.transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUw2q8W5WHTH"
      },
      "outputs": [],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "X_train[\"Seasons\"] = le.fit_transform(X_train[\"Seasons\"])\n",
        "X_train[\"Holiday\"] = le.fit_transform(X_train[\"Holiday\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train[\"Seasons\"] "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Label encoder asigna un numero a cada categoria. Esto sirve para los algoritmos de aprendizaje automático que generalmente requieren que las entradas sean numéricas. Para utilizarlo primero se importa la libreria preprocessing, luego se instancia el objeto LabelEncoder() y luego se utiliza fit_transform(columna) en la varibale que queremos transformar. Es importante tener en cuenta que el Label Encoding solo es adecuado para variables categóricas sin orden específico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvvSWpj59jwR"
      },
      "source": [
        "> c) Aplique la transformación aprendida por el Label Encoder al conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print( X_test[\"Seasons\"].unique() )\n",
        "print( X_train[\"Seasons\"].unique() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ3FKsTCKWk9"
      },
      "outputs": [],
      "source": [
        "X_test[\"Seasons\"] = le.transform(X_test[\"Seasons\"])\n",
        "X_test[\"Holiday\"] = le.transform(X_test[\"Holiday\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "susLNAFn9zBw"
      },
      "source": [
        "> d) ¿Por qué no se aplica la transformación del Label Encoder directamente sobre el conjunto de datos completo?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Porque label encoder sirve para variables categoricas sin orden y el conjunto de datos contiene distintos tipos de variables, lo cual no seria practico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAi_kFUa5RWh"
      },
      "source": [
        "## 1.2 Modelos de Regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCKBHS4B6ZfT"
      },
      "source": [
        "Primero entrenaremos un regresor lineal con los datos _en bruto_, ya que básicamente se transformaron las variables _Date_, _Seasons_ y _Holiday_ tal que todos los datos se puedan procesar por el regresor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WehFicHT_do_"
      },
      "source": [
        "### 1.2.1 Modelo en bruto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY7LByBqDw2C"
      },
      "source": [
        "> a) Utilice LinearRegression para entrenar el regresor y posteriormente obtener predicciones utilizando el testset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se8UjYIuD4Sh"
      },
      "outputs": [],
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Mb_OjkyU6h"
      },
      "source": [
        "> b) Obtenga el error cuadrático medio de la predicción sobre los datos del conjunto *X_test*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1mUfVDdLLb4"
      },
      "outputs": [],
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "mean_squared_error(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHANfCdoyxUW"
      },
      "source": [
        "> c) Grafique los resultados de la predicción (y_pred) versus los target de la prueba (y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REjrBtlbywtI"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_pred)\n",
        "\n",
        "plt.xlabel('Test')\n",
        "plt.ylabel('Prediction')\n",
        "plt.title('Test vs Prediction')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eCbIBCuCPkK"
      },
      "source": [
        "### 1.2.2 Modelo preprocesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlrJqqF1Ggv5"
      },
      "source": [
        "Variables cíclicas\n",
        "\n",
        "Si aplicaramos variables _Dummy_ o transformar con _One Hot Encoder_ a la fecha terminaríamos con 12 columnas para los meses, 7 columnas para los días de la semana y 24 columnas para la hora! En lugar de eso nos aprovecharemos de que todas estas variables son cíclicas, es decir, después de 23:00 hrs viene 00:00 hrs y el ciclo se vuelve a repetir, lo mismo para los meses del año y los días de la semana. Entonces podemos pensar en este ciclo como si fuera un _circulo_ utilizando el seno y el coseno de la fecha tenemos las coordenada de la fecha en el _circulo_. De esta forma las 23:00 y las 00:00 son cercanas en esta representación. Gracias a lo anterior nos quedamos con solo 2 columnas.\n",
        "\n",
        "$\\text{var_cos} = \\cos(2π\\; \\text{var}/\\text{periodo})$\n",
        "\n",
        "$\\text{var_sin} = \\sin(2π\\; \\text{var}/\\text{periodo})$\n",
        "\n",
        "> a) ¿Por qué no simplemente usar solo el seno o solo el coseno?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34gOfsesG_bi"
      },
      "outputs": [],
      "source": [
        "X_train['month_sin'] = np.sin(2 * np.pi * X_train['month'] / 12)\n",
        "X_train['month_cos'] = np.cos(2 * np.pi * X_train['month'] / 12)\n",
        "X_train['hour_sin'] = np.sin(2 * np.pi * X_train['Hour'] / ?)\n",
        "X_train['hour_cos'] = ...\n",
        "\n",
        "X_test['month_sin'] = np.sin(2 * np.pi * X_test['month'] / 12)\n",
        "X_test['month_cos'] = np.cos(2 * np.pi * X_test['month'] / 12)\n",
        "X_test['hour_sin'] = np.sin(2 * np.pi * X_test['Hour'] / ?)\n",
        "X_test['hour_cos'] = ...\n",
        "\n",
        "X_train.drop(['month', 'Hour', 'dayofweek'], axis=1, inplace=True)\n",
        "X_test.drop(['month', 'Hour', 'dayofweek'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu16autsXW4F"
      },
      "source": [
        "Estación\n",
        "\n",
        "> b) Aplique la misma lógica de variable _cíclica_ a la variable _Season_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLgmspozXS4P"
      },
      "outputs": [],
      "source": [
        "X_train['Sin_Season'] = np.sin(2 * np.pi * X_train['Seasons'] / ?) \n",
        "X_train['Cos_Season'] = np.cos(2 * np.pi * X_train['Seasons'] / ?)\n",
        "X_train.drop('Seasons', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc2R0j-4SVgV"
      },
      "outputs": [],
      "source": [
        "X_test['Sin_Season'] = ...\n",
        "X_test['Cos_Season'] = ...\n",
        "X_test.drop('Seasons', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq2dyE2XYExy"
      },
      "source": [
        "Normalización de variables.\n",
        "\n",
        "> c) Transforme las variables para que estas sigan una distribución normal con media 0 y varianza 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KclpuXuMU9mL"
      },
      "outputs": [],
      "source": [
        "X_scaler = preprocessing.StandardScaler()\n",
        "y_scaler = preprocessing.StandardScaler()\n",
        "\n",
        "\n",
        "cols_to_normalize = ['Temperature(°C)', \n",
        "                     'Humidity(%)', \n",
        "                     'Wind speed (m/s)', \n",
        "                     'Visibility (10m)', \n",
        "                     'Dew point temperature(°C)', \n",
        "                     'Solar Radiation (MJ/m2)',\t\n",
        "                     'Rainfall(mm)',\t\n",
        "                     'Snowfall (cm)',\n",
        "                     'year']\n",
        "\n",
        "X_train[cols_to_normalize] = X_scaler.fit_transform(X_train[cols_to_normalize])\n",
        "y_train = y_scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "\n",
        "X_test[cols_to_normalize] = X_scaler.transform(X_test[cols_to_normalize])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmtA_D3WBWnU"
      },
      "source": [
        "> d) Utilice LinearRegression para entrenar el regresor y posteriormente obtener predicciones utilizando el testset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av26_PMxBFea"
      },
      "outputs": [],
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSMYcJftCxO3"
      },
      "source": [
        "> e) Obtenga la predicción del regresor ya entrenado sobre el conjunto de prueba, aplicando la transformación inversa para _volver a la escala original_ de bicicletas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytHAjHNcBpKR"
      },
      "outputs": [],
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "y_pred = y_scaler.inverse_transformy_pred\n",
        "\n",
        "\"\"\"\n",
        "Su código aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npa8kFv3OOAI"
      },
      "source": [
        "> f) Calcule el error cuadrático medio en la escala original de bicicletas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCbijFJzOMUu"
      },
      "outputs": [],
      "source": [
        "mean_squared_error(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX70Cn26CmaE"
      },
      "source": [
        "> g) ¿Mejoran los resultados con respecto al modelo entrenado sobre los datos en bruto?\n",
        "\n",
        "> h) Grafique los resultados de la predicción (y_pred) versus los target de la prueba (y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbhcGM-nDYiP"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_pred)\n",
        "\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.xlabel('Test')\n",
        "plt.ylabel('Prediction')\n",
        "plt.title('Test vs Prediction')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOEgddXyDft9"
      },
      "source": [
        "### 1.2.3 Modelo SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ6O1hIgDjfg"
      },
      "source": [
        "En esta pregunta aplicaremos el modelo de regresión lineal utilizando el gradiente descendente estocástico (SGD), y para visualizar su rendimiento observaremos el error cuadrático medio a medida que el algoritmo SGD va iterando a través de las _epochs_. Para ello necesitamos separar una porción del conjunto de entrenamiento y así obtener el error de validación del modelo en ejemplos que no ha visto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tbb4nm86cv2"
      },
      "source": [
        "> a) Separe el conjunto de entrenamiento en un subconjunto de entrenamiento y uno de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1ZY46sgYBMN"
      },
      "outputs": [],
      "source": [
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzLZfYdR7sOM"
      },
      "source": [
        "Utilize la librería _sklearn_ para entrenar el modelo a través _SGDRegressor_ y responda las siguientes preguntas:\n",
        "\n",
        "> b) ¿Para qué sirve cada uno de los parametros que acepta _SGDRegressor_? Describa brevemente cada uno.\n",
        "\n",
        "> c) Explique cuál es la función del parámetro alpha y compare, teóricamente, las variaciones de implementación del parámetro disponibles en el módulo, es decir, compare: _constant_ vs _optimal_ vs _invscaling_ vs _adaptive_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQsOzYwFFvAT"
      },
      "outputs": [],
      "source": [
        "reg = SGDRegressor(loss='squared_error', \n",
        "                   penalty=None,                    \n",
        "                   tol=0.001, \n",
        "                   shuffle=True,                                                          \n",
        "                   warm_start=True,\n",
        "                   ...)\n",
        "\"\"\"\n",
        "Su código aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAdSvmSeANcK"
      },
      "source": [
        "Entrene el regresor SGD por 100 _epochs_ como máximo, utilizando *early_stoping* con un a paciencia de 10 _epochs_. Grafique el error cuadrático medio por epoca y responda las siguientes preguntas:\n",
        "\n",
        "> d) ¿Para qué sirve el *early_stoping*?\n",
        "\n",
        "> e) ¿Cómo se interpretan los resultados del entrenamiento según las curvas obtenidas?\n",
        "\n",
        "> f) ¿Cuántas _epochs_ duró su entrenamiento?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91VOi7s8LA3t",
        "outputId": "a9ceb7b5-58a2-47af-93e9-40c061e7ce35"
      },
      "outputs": [],
      "source": [
        "y_tr = np.asarray(y_tr).ravel()\n",
        "y_test = np.asarray(y_test).ravel()\n",
        "\n",
        "tr_errors, val_errors = [], []\n",
        "\n",
        "# train the model using early stopping\n",
        "n_epochs = 100\n",
        "best_val_error = float('inf')\n",
        "patience = 10  # stop tring if validation error doesn't improve after 10 epochs\n",
        "epoch_since_best = 0\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    reg.partial_fit(X_tr, y_tr)\n",
        "    \n",
        "    # collect the train and validation errors after each epoch\n",
        "    y_tr_pred = reg.predict(X_tr)\n",
        "    y_val_pred = reg.predict(X_val)\n",
        "    tr_error = mean_squared_error(y_tr, y_tr_pred)\n",
        "    val_error = mean_squared_error(y_val, y_val_pred)\n",
        "    tr_errors.append(tr_error)\n",
        "    val_errors.append(val_error)\n",
        "    \n",
        "    # check if validation error has improved\n",
        "    if val_error < best_val_error:\n",
        "        best_val_error = val_error\n",
        "        epoch_since_best = 0\n",
        "    else:\n",
        "        epoch_since_best += 1\n",
        "        if epoch_since_best >= patience:\n",
        "            print(f\"Stopping training after {epoch+1} epochs\")\n",
        "            break    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH4VXigCOd75"
      },
      "outputs": [],
      "source": [
        "plt.plot(tr_errors, label='train error')\n",
        "plt.plot(val_errors, label='validation error')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSlESNsjFI3a"
      },
      "source": [
        "> g) Obtenga la predicción del regresor ya entrenado sobre el conjunto de prueba, aplicando la transformación inversa para _volver a la escala original_ de bicicletas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YER8G3EI8uK"
      },
      "outputs": [],
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "y_pred = y_scaler.inverse_transform(y_pred)\n",
        "\n",
        "\"\"\"\n",
        "Su código aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_DaYKo7SE9v"
      },
      "source": [
        "> h) Calcule el error cuadrático medio en la escala original de bicicletas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2D6yXCORWrB"
      },
      "outputs": [],
      "source": [
        "mean_squared_error(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS21beTzST-k"
      },
      "source": [
        "> i) ¿Mejoran los resultados con respecto al modelo anterior?\n",
        "\n",
        "> j) Grafique los resultados de la predicción (y_pred) versus los target de la prueba (y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lni-dzFsasoO"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_pred)\n",
        "\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.xlabel('Test')\n",
        "plt.ylabel('Prediction')\n",
        "plt.title('Test vs Prediction')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJCL43JRfk4J"
      },
      "source": [
        "## 1.3 Modelo con Regularización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoQTe0vNHmrK"
      },
      "source": [
        "### 1.3.1 Regularización Ridge y Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MWQDDF-Hyzk"
      },
      "source": [
        "Agregue regularización Ridge al modelo SGD y entrénelo por 100 _epochs_ como máximo, utilizando early_stoping con un a paciencia de 10 _epochs_. Grafique el error cuadrático medio por _epoch_ y responda las siguientes preguntas:\n",
        "\n",
        "> a) ¿Qué son los llamados _hiperparámetros_ de un modelo y como se relacionan con el concepto de _hyper tuning_?\n",
        "\n",
        "> b) ¿Cuál es la motivación de agregar una penalización al modelo?\n",
        "\n",
        "> c) ¿De qué se compone la penalización mediante regularización Ridge?\n",
        "\n",
        "> d) ¿Cómo funciona la regularización Lasso?\n",
        "\n",
        "> e) ¿Cuáles son las ventajas y desventajas de usar regularización Ridge versus Lasso?\n",
        "\n",
        "> f) Utilizando GridSearchCV optimize el hiperparámetro _learning rate_ para los valores 0.1, 0.01, 0.001.\n",
        "\n",
        "> g) Pruebe con distintos tipos de _learning rate_ 'constant', 'optimal', 'invscaling'.\n",
        "\n",
        "> h) Optimize también los hiperparámetros de coeficiente de regularización de l2 y l1 para los valores 0.0001, 0.001, 0.01.\n",
        "\n",
        "\n",
        "\n",
        "> **Nota:** Investigue y lea la documentación de SGDRegressor, de KFold y de GridSearchCV antes de modificar y ejecutar el código, recuerde que el código presentado en el notebook es solo una guía de referencia y no necesariamente funciona, usted puede modificar su propio código y explicar su funcionamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUBYCI0jL6Tz"
      },
      "outputs": [],
      "source": [
        "k=5\n",
        "cv_folds=KFold(n_splits=k, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4imROnlXdZhJ"
      },
      "outputs": [],
      "source": [
        "reg = SGDRegressor(loss='squared_error', \n",
        "                   penalty='l2', # regularization therm\n",
        "                   alpha=0.0001, # regularization coeficient\n",
        "                   max_iter=100, #epochs\n",
        "                   tol=0.001, \n",
        "                   shuffle=True,                                       \n",
        "                   learning_rate='invscaling', # type of lr\n",
        "                   eta0=0.01,    # learning rate coeficient\n",
        "                   power_t=0.25                   \n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_nzYGAWZGWV"
      },
      "outputs": [],
      "source": [
        "param_grid = {'eta0': [0.1, 0.01, 0.001], \n",
        "              'penalty': ['l2', 'l1']\n",
        "              'alpha': [0.0001, 0.001, 0.01],                \n",
        "              'learning_rate': ['constant', 'optimal', 'invscaling']}\n",
        "\n",
        "hypereg = GridSearchCV(reg, \n",
        "             param_grid, \n",
        "             scoring='neg_mean_squared_error',               \n",
        "             refit=True, \n",
        "             cv=cv_folds, \n",
        "             verbose=0, \n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddSWyyWGgb79"
      },
      "outputs": [],
      "source": [
        "hypereg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DDTqsCeTYCs"
      },
      "source": [
        "> i) Obtenga los resultados para cada hiperparámetro de la grilla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVw4SnJQTYUN"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(hypereg.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPKiSNTBYXot"
      },
      "source": [
        "> j) Obtenga la predicción del regresor ya entrenado sobre el conjunto de prueba, aplicando la transformación inversa para _volver a la escala original_ de bicicletas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uShQGyHHYX1-"
      },
      "outputs": [],
      "source": [
        "y_pred = hypereg.predict(X_test)\n",
        "\n",
        "y_pred = y_scaler.inverse_transform(y_pred)\n",
        "\n",
        "\"\"\"\n",
        "Su código aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usk2GDO-Tsmy"
      },
      "source": [
        "> k) Calcule el error cuadrático medio en la escala original de bicicletas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8H1CN0oTsxG"
      },
      "outputs": [],
      "source": [
        "mean_squared_error(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKdGZ5slT59v"
      },
      "source": [
        "> l) ¿Mejoran los resultados con respecto a los modelo anteriores?\n",
        "\n",
        "> m) Grafique los resultados de la predicción (y_pred) versus los target de la prueba (y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00mlzpdCT8lb"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_pred)\n",
        "\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.xlabel('Test')\n",
        "plt.ylabel('Prediction')\n",
        "plt.title('Test vs Prediction')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y6DNuw9qDQS"
      },
      "source": [
        "## 1.4 Conclusiones:\n",
        "\n",
        "(Escriba aquí sus conclusiones de la pregunta 1: Regresión lineal para la predicción de la demanda de bicicletas compartidas en Seúl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBCbHCN5r7S"
      },
      "source": [
        "# 2. Clasificación de hongos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZbj_ChKFwXo"
      },
      "source": [
        "\n",
        "\n",
        "La tarea de clasificación se refiere a un problema en el que se tiene un conjunto de datos y se quiere predecir la clase a la que pertenece cada elemento del conjunto de datos.\n",
        "\n",
        "Para hacer esto, utilizaríamos un algoritmo de clasificación que examina las características de cada fruta en el conjunto de datos y aprende patrones que pueden ser utilizados para predecir su clase.\n",
        "\n",
        "Hay muchos algoritmos de clasificación diferentes que se pueden utilizar en el aprendizaje automático, por ejemplo la regresión logística, el SVM (máquina de soporte vectorial), Random Forest y redes neuronales, etcétera.\n",
        "\n",
        "Una vez que se entrena el modelo, se utiliza para predecir la clase de nuevos datos que no se incluyeron en el conjunto de datos original. Para medir el desempeño del modelo, se pueden utilizar diferentes métricas de evaluación, como la precisión, el recall y la F1-score.\n",
        "\n",
        "La tarea de clasificación es muy útil en muchas áreas, como la medicina, la banca, el comercio electrónico y la publicidad, ya que permite la automatización de tareas que antes requerían una gran cantidad de tiempo y recursos humanos.\n",
        "\n",
        "<center><img src=https://w0.peakpx.com/wallpaper/496/595/HD-wallpaper-red-mushroom-mushroom-red-beautiful.jpg></center>\n",
        "\n",
        "Este conjunto de datos incluye descripciones de muestras correspondientes a 23 especies de hongos de la familia Agaricus y Hongo Lepiota extraídas de la Guía de campo de hongos norteamericanos de la Audubon Society. Cada especie se identifica como definitivamente comestible, definitivamente venenosa o de comestibilidad desconocida y no recomendada. Esta última clase se ha combinado con la venenosa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G3OZUw38FcJ"
      },
      "source": [
        "## 2.1 Manipulación y exploración del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPfpZ8zWFygZ",
        "outputId": "67ee96fa-0b17-402f-b277-fb4abd9b3fa8"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d uciml/mushroom-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTkr6KXFpWLz",
        "outputId": "35be0cc4-7af5-4124-bca4-d9b36f7d39c3"
      },
      "outputs": [],
      "source": [
        "!unzip mushroom-classification.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO2RzdBgovmC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/mushrooms.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeWH4Rtvo2eK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "zHMHTQQnqqts",
        "outputId": "b64a7e51-f19c-491a-b036-08f38e7d9425"
      },
      "outputs": [],
      "source": [
        "def datainfo():\n",
        "    temp_ps = pd.DataFrame(index=df.columns)\n",
        "    temp_ps['DataType'] = df.dtypes\n",
        "    temp_ps[\"Non-null_Values\"] = df.count()\n",
        "    temp_ps['Unique_Values'] = df.nunique()\n",
        "    temp_ps['NaN_Values'] = df.isnull().sum()\n",
        "    temp_ps['NaN_Values_Percentage'] = (temp_ps['NaN_Values']/len(df))*100 \n",
        "    return temp_ps\n",
        "\n",
        "datainfo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2kvhHwfygON"
      },
      "source": [
        "### 2.1.1 Exploración de features\n",
        "\n",
        "> a) ¿Por qué la variable _veil-type_ no aporta ningún valor al entrenamiento? Grafique, justifique y elimine la columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "CwH3fAB8uVrs",
        "outputId": "4b3c9cb9-c262-45af-b6d0-6192200e25ff"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=\"veil-type\", data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvp6YJH4zDRm"
      },
      "outputs": [],
      "source": [
        "df.drop('veil-type', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1RmGujEYLub"
      },
      "source": [
        "> b) Grafique la distribución de cada variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0HBjhQ8zK5n"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:    \n",
        "    sns.catplot(x=col, kind=\"count\", data=df, height=3, aspect=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A4vBrbujC62"
      },
      "source": [
        "> c) Separe el conjunto de datos en los subconjuntos de entrenamiento y prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2nEGvc3aUqJ"
      },
      "source": [
        "> d) Codifique las variables de entrada para que puedan ser trabajadas en las siguientes preguntas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24H4pse5D0rh"
      },
      "source": [
        "> e) Separe los subconjuntos en _X_ e _y_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpRGMbAmapJb",
        "outputId": "afca674e-67b7-41e7-8287-16dd689ccd6c"
      },
      "outputs": [],
      "source": [
        "# load data and split into train/test sets\n",
        "train_set = df.sample(frac=0.7, random_state=42)\n",
        "test_set = df.drop(train_set.index)\n",
        "\n",
        "# create a list to store the columns to be dropped\n",
        "drop_cols = []\n",
        "\n",
        "# iterate over columns and check for binary variables\n",
        "for col in train_set.columns:\n",
        "    if len(train_set[col].unique()) == 2:\n",
        "        # convert binary variables to 0/1\n",
        "        train_set[col] = pd.factorize(train_set[col])[0]\n",
        "        test_set[col] = pd.factorize(test_set[col])[0]\n",
        "    else:\n",
        "        # get dummies for categorical variables with more than 2 categories\n",
        "        train_set = pd.concat([train_set, pd.get_dummies(train_set[col], prefix=col)], axis=1)\n",
        "        test_set = pd.concat([test_set, pd.get_dummies(test_set[col], prefix=col)], axis=1)\n",
        "        # add the original column to the list of columns to be dropped\n",
        "        drop_cols.append(col)\n",
        "\n",
        "# drop the original categorical columns from the train and test sets\n",
        "train_set.drop(drop_cols, axis=1, inplace=True)\n",
        "test_set.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "# ensure that the train and test sets have the same columns\n",
        "train_set, test_set = train_set.align(test_set, join='outer', axis=1, fill_value=0)\n",
        "\n",
        "# print the shapes of the train and test sets\n",
        "print('Train set shape:', train_set.shape)\n",
        "print('Test set shape:', test_set.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFLinCmeFlJ7"
      },
      "outputs": [],
      "source": [
        "X_train = train_set.drop('class', axis=1)\n",
        "y_train = train_set['class']\n",
        "X_test = test_set.drop('class', axis=1)\n",
        "y_test = test_set['class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOZyIyBi8qem"
      },
      "source": [
        "### 2.1.2 Principal Component Analisis\n",
        "\n",
        "PCA (Principal Component Analysis) es una técnica de reducción de dimensionalidad comúnmente usada en machine learning para reducir el número de características en el conjunto de datos conservando la información más importante.\n",
        "\n",
        "> a) Aplique PCA a los datos preprocesados para reducir el número de características a un conjunto más pequeño de componentes principales que capturen los patrones más importantes de los datos. Pruebe con n_components = n_features - k, donde k={1,10,50}, es decir, cree 3 posibles transformaciones de \"*X_train*\" y apliquelas sobre \"*X_test*\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks2RQ2eMeT8U"
      },
      "source": [
        "**Nota:** En adelante se le llamará \"representación de los datos\" a las distintas _feature selection/extraction_, por ejemplo: una posible representación de los datos es aplicar PCA con las 80 componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G2xFi8So5-H"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=#n_features - {1,10,50}\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "\"\"\"\n",
        "Su código aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3rcLNkJ8Mun"
      },
      "source": [
        "### 2.1.3 Mutual Information\n",
        "\n",
        "La información mutua es una medida de dependencia entre variables, se puede usar para medir el grado de asociación entre las _features_ y el _target_, y usar esta información para seleccionar las _features_ más relevantes para posteriormente realizar la clasificación. Al utilizar la información mutua para seleccionar las características más informativas, puede mejorar potencialmente el rendimiento de su modelo de aprendizaje automático reduciendo la cantidad de información irrelevante o redundante en el conjunto de datos.\n",
        "\n",
        "> a) Calcule la información mutua entre cada característica y la variable objetivo (comestible o venenoso) para medir el grado de asociación entre cada característica y la variable objetivo.\n",
        "\n",
        "> b) Seleccione las n características con las puntuaciones de información mutua más altas como entrada para un modelo de aprendizaje automático. Pruebe con n = n_features - k, para k={1,10,50}, es decir, cree 3 posibles transformaciones de \"X_train\" y apliquelas sobre \"X_test\".\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqINeaZ3MdOJ"
      },
      "outputs": [],
      "source": [
        "mi =mutual_info_classif(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akk4t7pyNNBF"
      },
      "outputs": [],
      "source": [
        "# set k to the number of features you want to exclude\n",
        "k = 1, 10, 50\n",
        "\n",
        "for i in k:\n",
        "  # compute the number of features to select\n",
        "  n = len(mi) - i\n",
        "\n",
        "  # get the indices of the features sorted by mutual information score\n",
        "  sorted_indices = np.argsort(mi)\n",
        "\n",
        "  # select the top n features\n",
        "  selected_indices = sorted_indices[-n:]\n",
        "\n",
        "  ...\n",
        "  \"\"\"\n",
        "  Su código aquí\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um0CQ3Qv86Jz"
      },
      "source": [
        "## 2.2 Modelos de clasificación\n",
        "\n",
        "Primero entrenaremos el modelo Naïve Bayes con los conjuntos de datos de entrenamiento generados en el punto 2.1, luego entrenaremos otros clasificadores utilizando como conjunto de entrenamiento la mejor representación de los datos según las metricas de clasificación obtenidas con el modelo Naïve Bayes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP7cUph-H4_p"
      },
      "source": [
        "### 2.2.1 Naïve Bayes\n",
        "\n",
        "Naïve Bayes es un modelo generativo clásico para la clasificación en el aprendizaje automático. En el contexto de la clasificación de hongos, puede utilizar Naïve Bayes para modelar la distribución de probabilidad de cada característica dada la etiqueta de clase (comestible o venenosa) y utilizar la regla de Bayes para calcular la probabilidad posterior de cada etiqueta de clase dadas las características observadas.\n",
        "\n",
        "Al utilizar Naïve Bayes para la clasificación, se puede conseguir un buen rendimiento con cantidades relativamente pequeñas de datos de entrenamiento y modelos relativamente sencillos. Sin embargo, Naïve Bayes asume que las características son condicionalmente independientes dada la etiqueta de clase, lo que puede no ser cierto en la práctica. No obstante, Naïve Bayes puede ser un modelo _baseline_ útil para comparar con modelos más complejos.\n",
        "\n",
        "Dado el conjuntos de entrenamiento con características observadas, se utiliza la regla de Bayes para calcular la probabilidad a posteriori de cada etiqueta de clase dadas las características observadas. La etiqueta de clase con la probabilidad a posteriori más alta sería la etiqueta de clase predicha para la muestra de hongos.\n",
        "\n",
        "Si consideramos que $P(x_1, \\dots, x_n)$ es constante, entonces:\n",
        "\n",
        "\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y).\\end{aligned}\\end{align}\n",
        "\n",
        "Usted deberá utilizar la estimación Máxima A Posteriori (MAP) para estimar $P(y)$ y $P(x_i \\mid y)$, donde $P(y)$ es la frecuancia relativa de la clase $y$ y $P(x_i \\mid y)$ asumiremos que se distribuye de forma Gaussiana, es decir:\n",
        "\n",
        "\\begin{align}\\begin{aligned}P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\\end{aligned}\\end{align}\n",
        "\n",
        "> a) ¿Qué otras distribuciones de probabilidad se pueden asumir para este problema considerando los valores posibles de las _features_ y del target en el conjunto de datos? Nombre mínimo 2.\n",
        "\n",
        "Para las preguntas b) y c) considere que los datos siguen una distribución de probabilidad Gaussiana y entrene el clasificador utilizando el módulo _GaussianNB_ de la librería _sklearn_.\n",
        "\n",
        "> b) Entrene e clasificador Naïve Bayes con los datos en _bruto_, es decir, tal cual como vienen, sin haber aplicado ninguna extracción o selección de características.\n",
        "\n",
        "> c) Entrene e clasificador Naïve Bayes con cada una de las representaciones obtenidas en el punto 2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf1Dtw5HPc3D"
      },
      "outputs": [],
      "source": [
        "clf = GaussianNB()\n",
        "clf.fit(X_train_..., y_train_...)\n",
        "\n",
        "y_pred = clf.predict(X_test_...)\n",
        "\n",
        "\"\"\"\n",
        "Su códig aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reBuUAOAZRji"
      },
      "source": [
        "> d) Para cada representación de los datos del punto 2.1, obtenga las metricas de clasificación utilizando *classification_report* de la librería sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gil4sig8bTLq"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test_..., y_pred, labels=['edible', 'poisonous'], ...))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSbWWmEGdl0Q"
      },
      "source": [
        "> e) Determine la mejor representación de los datos según las metricas obtenidas en la pregunta anterior y utilice esa representación de aquí en adelante para entrenar a los otros clasificadores.\n",
        "\n",
        "> f) Muestre los errores tipo 1 y tipo 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxBQ4ho8pL3S"
      },
      "outputs": [],
      "source": [
        "# assume you have a classification report saved in the variable `report`\n",
        "cm = confusion_matrix(y_true, y_pred)  # replace y_true and y_pred with your actual true and predicted labels\n",
        "\n",
        "# extract the true positive, false positive, false negative, and true negative counts from the confusion matrix\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# create a 2x2 matrix of the type 1 and type 2 errors\n",
        "error_matrix = [[fp, fn], [tn, tp]]\n",
        "\n",
        "# plot the heatmap using seaborn\n",
        "sns.heatmap(error_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=['Type 1', 'Type 2'], yticklabels=['Type 1', 'Type 2'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBsq1Fz79PYq"
      },
      "source": [
        "### 2.2.2 Linear Discriminant Analisis\n",
        "\n",
        "El análisis discriminante lineal (LDA) es un algoritmo clásico de clasificación lineal, y su objetivo es encontrar una combinación lineal de características que maximice la separación entre las dos clases (setas comestibles y venenosas). En otras palabras, el LDA intenta proyectar los datos en un espacio de menor dimensión, preservando al mismo tiempo la mayor discriminación de clases posible.\n",
        "\n",
        "La etiqueta de clase con la media más cercana en el espacio transformado sería la etiqueta de clase predicha para la muestra de setas.\n",
        "\n",
        "> a) ¿Qué es lo que asume el modelo LDA sobre la distribución condicional de las clases?\n",
        "\n",
        "> b) ¿Qué es lo que asume el modelo LDA sobre las matrices de covarianza de la distribución condicional?\n",
        "\n",
        "> c) Entrene este clasificador con el mejor dataset del punto 2.1 según los resultados obtenidos con el clasificador Naïve Bayes en el punto 2.2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxrmNpGNWWoS"
      },
      "outputs": [],
      "source": [
        "clf = LinearDiscriminantAnalysis(n_components=)\n",
        "clf.fit(X_train_..., y_train_...)\n",
        "\n",
        "y_pred = clf.predict(X_test_...)\n",
        "\n",
        "\"\"\"\n",
        "Su códig aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLo4nrTbgoZh"
      },
      "source": [
        "> d) Obtenga las metricas de clasificación utilizando *classification_report* de la librería sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-6ME0RNgq-g"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test_..., y_pred, labels=['edible', 'poisonous'], ...))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J1Rk1aApr84"
      },
      "source": [
        "> e) Muestre los errores tipo 1 y tipo 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX4iJOCBpwQj"
      },
      "outputs": [],
      "source": [
        "# assume you have a classification report saved in the variable `report`\n",
        "cm = confusion_matrix(y_true, y_pred)  # replace y_true and y_pred with your actual true and predicted labels\n",
        "\n",
        "# extract the true positive, false positive, false negative, and true negative counts from the confusion matrix\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# create a 2x2 matrix of the type 1 and type 2 errors\n",
        "error_matrix = [[fp, fn], [tn, tp]]\n",
        "\n",
        "# plot the heatmap using seaborn\n",
        "sns.heatmap(error_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=['Type 1', 'Type 2'], yticklabels=['Type 1', 'Type 2'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgXLc7RX8-8k"
      },
      "source": [
        "### 2.2.3 Regresión logística\n",
        "\n",
        "La regresión logística es un modelo discriminativo de clasificación clásico que puede utilizarse para la clasificación de hongos. El objetivo de la regresión logística es encontrar el modelo lineal que mejor se ajuste y que pueda predecir la probabilidad de que cada muestra de hongo pertenezca a una clase determinada (comestible o venenosa).\n",
        "\n",
        "> a) ¿En qué consiste la estimación de máxima verosimilitud de coeficientes?\n",
        "\n",
        "> b) ¿Qué asume este modelo de la relación entre las características y la variable objetivo?\n",
        "\n",
        "> c) ¿Qué se hace para evitar el sobreajuste de este modelo?\n",
        "\n",
        "> d) Entrene este clasificador con el mejor dataset del punto 2.1 según los resultados obtenidos con el clasificador Naïve Bayes en el punto 2.2.1.\n",
        "\n",
        "**Nota:** Usted puede variar o probar con distintos hiperparámetros siempre y cuando se utilizen de forma correcta el modelo y los datos para que pueda obtener predicciones comparables con los otros modelos. Puede basarse en los hiperparámetros probados con el regresor logistico y en la documentación de la librería."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faocESHchCK_"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(...)\n",
        "clf.fit(X_train_..., y_train_...)\n",
        "\n",
        "y_pred = clf.predict(X_test_...)\n",
        "\n",
        "\"\"\"\n",
        "Su códig aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gk5_iFrhE-r"
      },
      "source": [
        "> e) Obtenga las metricas de clasificación utilizando *classification_report* de la librería sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US3MfxZGhHQN"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test_..., y_pred, labels=['edible', 'poisonous'], ...))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqY0jMAHpyB0"
      },
      "source": [
        "> f) Muestre los errores tipo 1 y tipo 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz2Wy17up2SU"
      },
      "outputs": [],
      "source": [
        "# assume you have a classification report saved in the variable `report`\n",
        "cm = confusion_matrix(y_true, y_pred)  # replace y_true and y_pred with your actual true and predicted labels\n",
        "\n",
        "# extract the true positive, false positive, false negative, and true negative counts from the confusion matrix\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# create a 2x2 matrix of the type 1 and type 2 errors\n",
        "error_matrix = [[fp, fn], [tn, tp]]\n",
        "\n",
        "# plot the heatmap using seaborn\n",
        "sns.heatmap(error_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=['Type 1', 'Type 2'], yticklabels=['Type 1', 'Type 2'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwRKRvP_9KIs"
      },
      "source": [
        "### 2.2.4 Support Vector Machines\n",
        "\n",
        "Las máquinas de vectores soporte (SVM) son un algoritmo de aprendizaje automático potente y versátil que puede utilizarse para la clasificación de hongos. La idea principal detrás de SVM es encontrar el hiperplano que mejor separa las diferentes clases en el conjunto de datos.\n",
        "\n",
        "Utilice los datos de entrenamiento para ajustar un modelo SVM a los datos. Esto implica encontrar el hiperplano que maximice el margen entre las muestras positivas y negativas. El margen es la distancia entre el hiperplano y las muestras más cercanas de cada clase.\n",
        "\n",
        "Si los datos no se pueden separar linealmente, utilice funciones de kernel para transformar los datos en un espacio de mayor dimensión en el que se puedan separar. Las funciones kernel más comunes incluyen funciones polinómicas, de base radial (RBF) y sigmoidales.\n",
        "\n",
        "Entrene un clasificador SVM con el mejor dataset del punto 2.1 según los resultados obtenidos con el clasificador Naïve Bayes en el punto 2.2.1.\n",
        "\n",
        "Entrene el clasificador con los siguientes tipos de _kernel_:\n",
        "\n",
        "> a) Lineal, con 2 valores del coeficiente gamma distintos.\n",
        "\n",
        "> b) Polinómico, con 2 valores del coeficiente gamma distintos.\n",
        "\n",
        "> c) rbf, con 2 valores del coeficiente gamma distintos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKtqj_fFDokx"
      },
      "outputs": [],
      "source": [
        "clf = NuSVC(kernel={'linear', 'poly', 'rbf'}, gamma=...)\n",
        "clf.fit(X_train_..., y_train_...)\n",
        "\n",
        "y_pred = clf.predict(X_test_...)\n",
        "\n",
        "\"\"\"\n",
        "Su códig aquí\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJhlgd3mUS3"
      },
      "source": [
        "> d) Obtenga las metricas de clasificación utilizando *classification_report* de la librería sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYGDlhIXmah5"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test_..., y_pred, labels=['edible', 'poisonous'], ...))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8kgyvVIp4Q-"
      },
      "source": [
        "> e) Muestre los errores tipo 1 y tipo 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iosBy5v1p70p"
      },
      "outputs": [],
      "source": [
        "# assume you have a classification report saved in the variable `report`\n",
        "cm = confusion_matrix(y_true, y_pred)  # replace y_true and y_pred with your actual true and predicted labels\n",
        "\n",
        "# extract the true positive, false positive, false negative, and true negative counts from the confusion matrix\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# create a 2x2 matrix of the type 1 and type 2 errors\n",
        "error_matrix = [[fp, fn], [tn, tp]]\n",
        "\n",
        "# plot the heatmap using seaborn\n",
        "sns.heatmap(error_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=['Type 1', 'Type 2'], yticklabels=['Type 1', 'Type 2'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7owu3oyXp8Zf"
      },
      "source": [
        "## 2.3 Conclusiones:\n",
        "\n",
        "(Escriba aquí sus conclusiones de la pregunta 2: Clasificación de hongos)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
